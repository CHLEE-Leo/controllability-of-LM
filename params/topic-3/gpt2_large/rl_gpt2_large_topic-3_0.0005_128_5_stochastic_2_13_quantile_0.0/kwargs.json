{"task": "rl", "behavior_model": "gpt2_large", "dataset": "topic-3", "rl_lr": 0.0005, "batch_size": 128, "num_epoch": 5, "decoding": "stochastic", "prefix_len": 2, "gen_len": 13, "dropout": "quantile", "dropout_rate": 0.0}